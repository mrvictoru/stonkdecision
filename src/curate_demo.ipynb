{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook demonstrate how to cuate dataset with the utitlity function provided\n",
    "# it involved two steps, first we create or train appropriate agent, second is to sample those agents interaction with the environment and store them in a dataset\n",
    "\n",
    "# Let's get the stock ticker we would use for curating the dataset\n",
    "\n",
    "# this library is to get the list of tickers from NASDAQ and DOWJONES\n",
    "from gettickers import *\n",
    "nasdaq_tic = get_nasdaq_tickers()\n",
    "dow_tic = get_dow_tickers()\n",
    "\n",
    "# merge the lists and remove duplicates\n",
    "all_tickers = nasdaq_tic + dow_tic\n",
    "all_tickers = list(set(all_tickers))\n",
    "\n",
    "# let's get the first 10 tickers and check the number of tickers in the lists\n",
    "print(all_tickers[:10])\n",
    "print(len(nasdaq_tic))\n",
    "print(len(dow_tic))\n",
    "print(len(all_tickers))\n",
    "\n",
    "# agents can be created from RL (stable-baselines3)\n",
    "# set date range for stock data to train RL-agents\n",
    "\n",
    "# this will set  the parameter for the training of the agents\n",
    "start_date = \"2017-01-01\"\n",
    "num_days = 400\n",
    "interval = \"1d\"\n",
    "indicators = [\"Volume\", \"volume_cmf\", \"trend_macd\", \"momentum_rsi\", \"momentum_stoch_rsi\", \"trend_sma_fast\"]\n",
    "init_balance = 20000\n",
    "agent_output_path = \"trained_stable_agents/\"\n",
    "stable_training_json_path = \"training_stable_agents_config/\"\n",
    "\n",
    "# save these parameter as json config file using the utility function from getsttickers\n",
    "create_json_files(all_tickers, start_date, num_days, interval, indicators, init_balance, agent_output_path, stable_training_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read one of the json file\n",
    "import os\n",
    "import json\n",
    "json_file_path = os.listdir(stable_training_json_path)[0]\n",
    "with open(os.path.join(stable_training_json_path, json_file_path), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(config)\n",
    "\n",
    "# extract the configuration parameters\n",
    "stock_name = config['stock_name']\n",
    "start_date = config['start_date']\n",
    "num_days = config['num_days']\n",
    "interval = config['interval']\n",
    "indicators = config['indicators']\n",
    "init_balance = config['init_balance']\n",
    "output_path = config['output_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are utility functions in curatedataset to help make gymnasium environment from the stock data\n",
    "from curatedataset import makegymenv\n",
    "stable_env, obs_space, act_space, col, data = makegymenv(stock_name=stock_name, start_date=start_date, period=num_days, interval=interval, indicators=indicators, normalize=False, init_balance=init_balance)\n",
    "\n",
    "# Stable-baselines3 library can be used to create and train the agents\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "num_cpu = 6\n",
    "# vectorized environment\n",
    "env_vec = SubprocVecEnv([lambda: stable_env for i in range(num_cpu)])\n",
    "# create the agent\n",
    "modelA2C = A2C(\"MlpPolicy\", env_vec, verbose=1)\n",
    "# we can evaluate the agent before training\n",
    "mean_reward, std_reward = evaluate_policy(modelA2C, Monitor(env_vec, n_eval_episodes=10, deterministic=False))\n",
    "print(f\"Pre-training -> mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# train the agent\n",
    "modelA2C.learn(total_timesteps=len(data)*80)\n",
    "\n",
    "# evaluate the agent after training\n",
    "mean_reward, std_reward = evaluate_policy(modelA2C, Monitor(env_vec, n_eval_episodes=10, deterministic=False))\n",
    "print(f\"Post training -> mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the steps above can be used to train multiple agents and store them in a folder with the utility function in get_agent.py\n",
    "from train_stable_agent import full_run\n",
    "\n",
    "json_files = os.listdir(stable_training_json_path)\n",
    "\n",
    "# loop through all json file and train stable-baseline agent with utility function\n",
    "for file in json_files:\n",
    "    # check if file ends with json\n",
    "    if file.endswith(\".json\"):\n",
    "        runfile = os.path.join(stable_training_json_path, file)\n",
    "        print(f\"Training agent with {runfile}\")\n",
    "        with open(runfile, \"r\") as f:\n",
    "            runconfig = json.load(f)\n",
    "        output_path = runconfig[\"output_path\"]\n",
    "        if os.path.exists(output_path) and len(os.listdir(output_path)) == 3:\n",
    "            print(f\"Agent with {runfile} already trained\")\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                full_run(runfile)\n",
    "            except Exception as e:\n",
    "                print(f\"Error training agent with {runfile}\")\n",
    "                print(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that stable-baseline agent is trained, we can go and sample them in the stock trading environment with different dates\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "# calculate the new start_date as the start_date + num_days for training stable-baseline agent\n",
    "start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "new_start_date = start_date + timedelta(days=num_days)\n",
    "\n",
    "# we want to sample the agents for the days different between start date and now (today) minus 120 days (those 120 days for testing)\n",
    "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "today = datetime.strptime(today, \"%Y-%m-%d\")\n",
    "day_diff = (today - new_start_date).days\n",
    "\n",
    "if day_diff < 120:\n",
    "    num_days = day_diff\n",
    "else:\n",
    "    num_days = day_diff - 120\n",
    "\n",
    "new_start_date = new_start_date.strftime(\"%Y-%m-%d\")\n",
    "data_output_path = \"offline_stock_trade_data/\"\n",
    "data_json_path = \"offline_stock_trade_data_config/\"\n",
    "\n",
    "# save these parameter as json config file using the utility function from getsttickers\n",
    "create_json_files(all_tickers, new_start_date, num_days, interval, indicators, init_balance, data_output_path, data_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read one of the json file\n",
    "json_file_path = os.listdir(data_json_path)[0]\n",
    "\n",
    "with open(os.path.join(data_json_path, json_file_path), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(config)\n",
    "\n",
    "# extract the configuration parameters\n",
    "stock_name = config['stock_name']\n",
    "start_date = config['start_date']\n",
    "num_days = config['num_days']\n",
    "interval = config['interval']\n",
    "indicators = config['indicators']\n",
    "init_balance = config['init_balance']\n",
    "output_path = config['output_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the gymnasium environment\n",
    "data_env, obs_space, act_space, col, data = makegymenv(stock_name=stock_name, start_date=start_date, period=num_days, interval=interval, indicators=indicators, normalize=False, init_balance=init_balance)\n",
    "# get the list of date from the data\n",
    "env_date = data.index.strftime(\"%Y-%m-%d\").tolist()\n",
    "# get the folder where the trained agents are stored\n",
    "agent_folder = os.path.join(agent_output_path, stock_name)\n",
    "# get the a2c agent path\n",
    "for file in os.listdir(agent_folder):\n",
    "    if file.endswith(\".zip\") and \"a2c\" in file:\n",
    "        a2c_agent_path = os.path.join(agent_folder, file)\n",
    "        break\n",
    "agent_type = (data_env, \"stable-baselines-a2c\", a2c_agent_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
