{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook is to test run the custom stock trading environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sentiment analysis\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import datetime as dt\n",
    "\n",
    "# set up tokenizer and model for sentiment analysis\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from getstock import get_newsheadline_sentiment, get_stock_data_yf_between_with_indicators_news\n",
    "# test get_newsheadline_sentiment and get__stock_data_with_news with TSLA, and the last 3 days\n",
    "indicators = [\"Volume\", \"volume_cmf\", \"trend_macd\", \"momentum_rsi\", \"momentum_stoch_rsi\", \"trend_sma_fast\"]\n",
    "symbol = 'TSLA'\n",
    "#end_date = dt.datetime.now()\n",
    "#start_date = end_date - dt.timedelta(days=3)\n",
    "\n",
    "# set start time to be 2022-01-01 and end time to be 2022-01-02\n",
    "start_date = dt.datetime(2022, 1, 1)\n",
    "end_date = dt.datetime(2022, 3, 1)\n",
    "start_date_string = start_date.strftime(\"%Y-%m-%d\")\n",
    "end_date_string = end_date.strftime(\"%Y-%m-%d\")\n",
    "#result = get_newsheadline_sentiment(symbol, start_date, end_date, device, tokenizer, model)\n",
    "data = get_stock_data_yf_between_with_indicators_news(symbol, start_date_string, end_date_string, \"1d\", indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this library is to get the list of tickers from NASDAQ and S&P500\n",
    "from gettickers import *\n",
    "\n",
    "nasdaq_tic = get_nasdaq_tickers()\n",
    "#sp500_tic = get_sp500_tickers()\n",
    "dow_tic = get_dow_tickers()\n",
    "#nyse_tic = get_nyse_tickers()\n",
    "#dow_tic = ['AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM', 'CSCO', 'CVX', 'DIS', 'DOW', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT']\n",
    "\n",
    "# merge the lists and remove duplicates\n",
    "all_tickers = nasdaq_tic + dow_tic\n",
    "all_tickers = list(set(all_tickers))\n",
    "print(len(nasdaq_tic))\n",
    "#print(len(sp500_tic))\n",
    "print(len(dow_tic))\n",
    "print(len(all_tickers))\n",
    "# loop through tickers and only keep the ones with capital letters\n",
    "# this is to remove the tickers with special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json files for all tickers for stable_baselines agent training\n",
    "start_date = \"2017-01-01\"\n",
    "num_days = 400\n",
    "interval = \"1d\"\n",
    "indicators = [\"Volume\", \"volume_cmf\", \"trend_macd\", \"momentum_rsi\", \"momentum_stoch_rsi\", \"trend_sma_fast\"]\n",
    "init_balance = 20000\n",
    "agent_output_path = \"trained_stable_agents_2/\"\n",
    "stable_training_json_path = \"training_stable_agents_config/\"\n",
    "#create_json_files(all_tickers, start_date, num_days, interval, indicators, init_balance, agent_output_path, stable_training_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json files for all tickers for decision transformer offline data generation\n",
    "from datetime import datetime, timedelta\n",
    "# calculate the new start_date as the above start_date + num_days\n",
    "start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "new_start_date = start_date + timedelta(days=num_days)\n",
    "\n",
    "# calculate the day different between current date and new_start_date\n",
    "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "today = datetime.strptime(today, \"%Y-%m-%d\")\n",
    "day_diff = (today - new_start_date).days\n",
    "\n",
    "if day_diff < 120:\n",
    "    num_days = day_diff\n",
    "else:\n",
    "    num_days = day_diff - 120\n",
    "\n",
    "new_start_date = new_start_date.strftime(\"%Y-%m-%d\")\n",
    "output_path = \"offline_stock_trade_data/\"\n",
    "data_json_path = \"offline_stock_trade_data_config/\"\n",
    "#create_json_files(all_tickers, new_start_date, num_days, interval, indicators, init_balance, output_path, data_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# import libraries for training stable baselines agent\n",
    "from train_stable_agent import create_stable_agents, evaluate_stable_agent, train_stable_agent, output_stable_agent, full_run\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# import custom functions and classes\n",
    "from curatedataset import makegymenv, run_env, full_curate_run\n",
    "\n",
    "from get_agent import Agent, TradingAlgorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open directory from json_path\n",
    "json_files = os.listdir(stable_training_json_path)\n",
    "\n",
    "# loop through all json file and train stablebaseline agent with full_run\n",
    "for file in json_files:\n",
    "    # check if file ends with json\n",
    "    if file.endswith(\".json\"):\n",
    "        runfile = os.path.join(stable_training_json_path, file)\n",
    "        print(f\"Training agent with {runfile}\")\n",
    "        with open(runfile, \"r\") as f:\n",
    "            runconfig = json.load(f)\n",
    "        output_path = runconfig[\"output_path\"]\n",
    "        if os.path.exists(output_path) and len(os.listdir(output_path)) == 3:\n",
    "            print(f\"Agent with {runfile} already trained\")\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                full_run(runfile)\n",
    "            except Exception as e:\n",
    "                print(f\"Error training agent with {runfile}\")\n",
    "                print(e)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "bad_count = 0\n",
    "good_count = 0\n",
    "\n",
    "# Loop through files in the output path and its subfolders\n",
    "for root, dirs, files in os.walk(agent_output_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".zip\"):\n",
    "            if \"bad\" in file:\n",
    "                bad_count += 1\n",
    "            elif \"good\" in file:\n",
    "                good_count += 1\n",
    "\n",
    "print(f\"Number of files with 'bad' in the name: {bad_count}\")\n",
    "print(f\"Number of files with 'good' in the name: {good_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json_files = os.listdir(data_json_path)\n",
    "\n",
    "# loop through all json file and run environment with trained stable agent and other algorithms\n",
    "for file in data_json_files:\n",
    "    # check if file ends with json\n",
    "    if file.endswith(\".json\"):\n",
    "        runfile = os.path.join(data_json_path, file)\n",
    "        print(f\"Curating data with {runfile}\")\n",
    "        # open the json file and read the content\n",
    "        with open(runfile, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        # get the ticker\n",
    "        ticker = data['stock_name']\n",
    "        output = data[\"output_path\"]\n",
    "        # check if the output path exists and if it contain data, if so skip\n",
    "        if os.path.exists(output) and len(os.listdir(output)) > 0:\n",
    "            print(f\"Data already curated for {ticker}\")\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                # joining the output path with ticker\n",
    "                trained_stable_agent_path = os.path.join(agent_output_path, ticker)\n",
    "                full_curate_run(runfile, trained_stable_agent_path, num_episodes=80)\n",
    "            except Exception as e:\n",
    "                print(f\"Error curating data with {runfile}\")\n",
    "                print(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curatedataset import calc_meanstd_datasets\n",
    "\n",
    "dataset_meanstd = calc_meanstd_datasets(output_path,['positive', 'negative', 'neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_decision_transformer import full_training_run, save_model\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model, trained_params = full_training_run(output_path, device = device, force_normalize=dataset_meanstd, n_epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nasdaq_dow_news_2'\n",
    "model_path = 'trained_nasdaq_dow_decision_transformer'\n",
    "save_model(trained_model, trained_params, model_name, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 7)\n",
      "┌────────────┬────────────┬────────────┬─────────────┬────────────┬───────────┬───────────┐\n",
      "│ reward_min ┆ reward_max ┆ reward_10  ┆ reward_25   ┆ reward_50  ┆ reward_75 ┆ reward_90 │\n",
      "│ ---        ┆ ---        ┆ ---        ┆ ---         ┆ ---        ┆ ---       ┆ ---       │\n",
      "│ f64        ┆ f64        ┆ f64        ┆ f64         ┆ f64        ┆ f64       ┆ f64       │\n",
      "╞════════════╪════════════╪════════════╪═════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ -589.13181 ┆ 465.624915 ┆ -198.15058 ┆ -125.586757 ┆ -64.339423 ┆ 7.249499  ┆ 88.915637 │\n",
      "└────────────┴────────────┴────────────┴─────────────┴────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "from curatedataset import eval_reward_datasets\n",
    "mean_rewards = eval_reward_datasets(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.24949879807864"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rewards['reward_75'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read normalized mean and std from model param\n",
    "import json\n",
    "import os\n",
    "from TradingEnvClass import MeanStdObject\n",
    "model_name = 'nasdaq_dow_news_2'\n",
    "model_path = 'trained_nasdaq_dow_decision_transformer'\n",
    "# get file ends with json\n",
    "json_files = [file for file in os.listdir(model_path) if file.endswith('params.json')]\n",
    "parameter_path = os.path.join(model_path, json_files[0])\n",
    "with open(parameter_path, 'r') as f:\n",
    "    normalize = json.load(f)[\"normalize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std_dict = {key: MeanStdObject(mean = value['mean'], std=value['std']) for key, value in normalize.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting stock data of TSLA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/usr/local/lib/python3.10/dist-packages/ta/trend.py:988: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '258.3132098388672' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  self._psar.iloc[i] = self._psar.iloc[i - 1] + (\n",
      "/usr/local/lib/python3.10/dist-packages/ta/trend.py:1030: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self._psar[i] = high2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting indicators...\n",
      "setting up sentiment analysis model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting news sentiment of TSLA...\n",
      "Getting stock data of TSLA completed.\n",
      "init env with max step:  250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/code/src/curatedataset.py:37: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  stock_data['momentum_stoch_rsi'].iloc[:10] = 0.5\n",
      "/code/src/curatedataset.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_data['momentum_stoch_rsi'].iloc[:10] = 0.5\n"
     ]
    }
   ],
   "source": [
    "from curatedataset import makegymenv, run_env\n",
    "test_tic = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'AMZN', 'META', 'NVDA', 'INTC', 'AMD']\n",
    "# make gym environment\n",
    "stock_name = test_tic[3]\n",
    "# TODO test different start date within the dataset \n",
    "start_date = '2022-06-01'\n",
    "num_days = 365\n",
    "interval = '1d'\n",
    "indicators = [\"Volume\", \"volume_cmf\", \"trend_macd\", \"momentum_rsi\", \"momentum_stoch_rsi\", \"trend_sma_fast\"]\n",
    "init_balance = 20000\n",
    "#data = get_stock_data_yf_between_with_indicators(stock_name, '2019-01-01', '2020-02-01', interval, ['all'])\n",
    "\n",
    "env,obs_space, act_space, col, data = makegymenv(stock_name, start_date, num_days, interval, indicators=indicators, normalize=mean_std_dict, init_balance=init_balance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_agent import Agent\n",
    "import torch\n",
    "import fnmatch\n",
    "# get model parameter's path under model path by checking for _params.json\n",
    "f = os.listdir(model_path)\n",
    "matching_files = fnmatch.filter(f, model_name+'*_params.json')\n",
    "\n",
    "# pick the 90th percentile reward as the target reward\n",
    "rtg_target = mean_rewards['reward_75'][0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DecisionTransformer = Agent(env, 'transformer', rtg_target=rtg_target, rtg_scale=0.7, model_path=parameter_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0 Timestep: 15  done\n",
      "Episode:  1 Timestep: 13  done\n",
      "Episode:  2 Timestep: 13  done\n",
      "Episode:  3 Timestep: 13  done\n",
      "Episode:  4 Timestep: 13  done\n",
      "Episode:  5 Timestep: 13  done\n",
      "Episode:  6 Timestep: 13  done\n",
      "Episode:  7 Timestep: 13  done\n",
      "Episode:  8 Timestep: 13  done\n",
      "Episode:  9 Timestep: 13  done\n"
     ]
    }
   ],
   "source": [
    "trade_data = run_env(DecisionTransformer, stock_name, env, 10, start_date, normalize_param = normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curatedataset import save_data\n",
    "file_name = os.path.join(model_path, 'trained_transformer_'+stock_name+'_'+start_date+'_test.json')\n",
    "save_data(trade_data, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sample sum reward:  -1623.830402306177\n",
      "Std sample sum reward:  0.0\n",
      "Mean net worth growth:  -1888.931640625\n",
      "Std net worth growth: +/- 0.0\n",
      "Plotting episode:  3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Index(...) must be called with a collection of some kind, Timestamp('2022-06-01 00:00:00') was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m eval_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_nasdaq_dow_decision_transformer/trained_transformer_TSLA_2022-06-01_test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# eval_data = file_name\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mevaluate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/code/src/curatedataset.py:281\u001b[0m, in \u001b[0;36mevaluate_dataset\u001b[0;34m(data_path)\u001b[0m\n\u001b[1;32m    279\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][episode][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_history\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    280\u001b[0m date \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m][:state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 281\u001b[0m \u001b[43mplot_stock_trading_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/code/src/StockTradingGraph.py:141\u001b[0m, in \u001b[0;36mplot_stock_trading_data\u001b[0;34m(state, col, action_history, date)\u001b[0m\n\u001b[1;32m    138\u001b[0m dt_index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(date)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# create a dataframe with the stock price data\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mOpen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHigh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mHigh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mLow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mClose\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# create an instance of the StockTradingGraph class\u001b[39;00m\n\u001b[1;32m    143\u001b[0m stock_graph \u001b[38;5;241m=\u001b[39m StockTradingGraph(df, \u001b[38;5;28;01mNone\u001b[39;00m, action_history, net_worth, windows_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:767\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    761\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    762\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    763\u001b[0m     )\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py:116\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    114\u001b[0m     index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m    119\u001b[0m arrays, refs \u001b[38;5;241m=\u001b[39m _homogenize(arrays, index, dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:7648\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7646\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   7647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:526\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    523\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(data):\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_scalar_data_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(np\u001b[38;5;241m.\u001b[39masarray(data), dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:5285\u001b[0m, in \u001b[0;36mIndex._raise_scalar_data_error\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m   5280\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   5281\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   5282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_scalar_data_error\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data):\n\u001b[1;32m   5283\u001b[0m     \u001b[38;5;66;03m# We return the TypeError so that we can raise it from the constructor\u001b[39;00m\n\u001b[1;32m   5284\u001b[0m     \u001b[38;5;66;03m#  in order to keep mypy happy\u001b[39;00m\n\u001b[0;32m-> 5285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   5286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(...) must be called with a collection of some \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(data)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(data,\u001b[38;5;250m \u001b[39mnp\u001b[38;5;241m.\u001b[39mgeneric)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5289\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Index(...) must be called with a collection of some kind, Timestamp('2022-06-01 00:00:00') was passed"
     ]
    }
   ],
   "source": [
    "from curatedataset import evaluate_dataset\n",
    "eval_data = \"trained_nasdaq_dow_decision_transformer/trained_transformer_TSLA_2022-06-01_test.json\"\n",
    "# eval_data = file_name\n",
    "evaluate_dataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
